{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5912b5da",
   "metadata": {},
   "source": [
    "## Spam Mail Detector (NLP Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6bc6de",
   "metadata": {},
   "source": [
    "### Spam Mail Detector using Enron Email Dataset\n",
    "\n",
    "#### Dataset (Kaggle)\n",
    "\n",
    "https://www.kaggle.com/datasets/marcelwiechmann/enron-spam-data\n",
    "\n",
    "Contains : 33716 real emails\n",
    "\n",
    "#### Project Objective\n",
    "The goal of this project is to build a Machine Learning classifier that can\n",
    "automatically distinguish between **spam** and **non-spam (ham)** emails using\n",
    "Natural Language Processing (NLP) techniques.\n",
    "\n",
    "#### Why This Project?\n",
    "Spam emails are a major security and productivity issue. By applying text\n",
    "preprocessing, feature extraction, and classification algorithms, we can\n",
    "automate spam detection efficiently.\n",
    "\n",
    "#### Techniques Used\n",
    "- Text preprocessing (tokenization, stopword removal)\n",
    "- TF-IDF feature extraction\n",
    "- Naive Bayes & Logistic Regression models\n",
    "- Performance evaluation using Accuracy, Precision, Recall, and F1-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a8f9ee",
   "metadata": {},
   "source": [
    "### 1. Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db62f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba1903b",
   "metadata": {},
   "source": [
    "### 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edfad334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam/Ham</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>vastar resources , inc .</td>\n",
       "      <td>gary , production from the high island larger ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>calpine daily gas nomination</td>\n",
       "      <td>- calpine daily gas nomination 1 . doc</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>re : issue</td>\n",
       "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>meter 7268 nov allocation</td>\n",
       "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Subject  \\\n",
       "0           0  christmas tree farm pictures   \n",
       "1           1      vastar resources , inc .   \n",
       "2           2  calpine daily gas nomination   \n",
       "3           3                    re : issue   \n",
       "4           4     meter 7268 nov allocation   \n",
       "\n",
       "                                             Message Spam/Ham        Date  \n",
       "0                                                NaN      ham  1999-12-10  \n",
       "1  gary , production from the high island larger ...      ham  1999-12-13  \n",
       "2             - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
       "3  fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
       "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"enron_spam_data.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1671f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33716, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26330aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33716 entries, 0 to 33715\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  33716 non-null  int64 \n",
      " 1   Subject     33716 non-null  object\n",
      " 2   Message     33664 non-null  object\n",
      " 3   Spam/Ham    33716 non-null  object\n",
      " 4   Date        33716 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74fe080",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c6cab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33716 entries, 0 to 33715\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Subject   33716 non-null  object\n",
      " 1   Message   33664 non-null  object\n",
      " 2   Spam/Ham  33716 non-null  object\n",
      " 3   Date      33716 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e312f",
   "metadata": {},
   "source": [
    "### 3. Handling Null Values and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e7f1289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subject      0\n",
       "Message     52\n",
       "Spam/Ham     0\n",
       "Date         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e4a58",
   "metadata": {},
   "source": [
    "#### Removes emails with no message body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1757d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['Message'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbccb603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subject     0\n",
       "Message     0\n",
       "Spam/Ham    0\n",
       "Date        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5bb8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(15436)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c6c63",
   "metadata": {},
   "source": [
    "#### Duplicate emails can bias the model by repeating the same patterns and inflating performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f6cba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337a6813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18228, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db15345",
   "metadata": {},
   "source": [
    "### 4. Data Preprocessing (Text Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76324735",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words and t not in string.punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "data['clean_text'] = data['Message'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ce43f",
   "metadata": {},
   "source": [
    "- Lowering reduces variability\n",
    "\n",
    "- Stopwords removal reduces noise\n",
    "\n",
    "- Tokenization breaks text into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb46c6",
   "metadata": {},
   "source": [
    "### 5. Feature Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b067cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data['clean_text'])\n",
    "y = data['Spam/Ham'].map({'ham':0, 'spam':1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff00dc",
   "metadata": {},
   "source": [
    "#### It converts text into numbers that ML models can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd37fa",
   "metadata": {},
   "source": [
    "### 6. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e947b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee9d11",
   "metadata": {},
   "source": [
    "### 7. Train Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21069450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb4dff",
   "metadata": {},
   "source": [
    "### 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c505649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9786066922654965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      3184\n",
      "           1       0.86      1.00      0.92       462\n",
      "\n",
      "    accuracy                           0.98      3646\n",
      "   macro avg       0.93      0.99      0.95      3646\n",
      "weighted avg       0.98      0.98      0.98      3646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fa1ee",
   "metadata": {},
   "source": [
    "### 9. Test on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "918dc00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: URGENT: Your bank account has been compromised. Verify here immediately.\n",
      "HAM probability : 0.9840\n",
      "SPAM probability: 0.0160\n",
      "Final Prediction: HAM\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# New email samples with stronger spam cues\n",
    "new_emails = [\n",
    "    \"URGENT: Your bank account has been compromised. Verify here immediately.\"\n",
    "]\n",
    "\n",
    "# Preprocess new emails\n",
    "clean_new_emails = [preprocess_text(email) for email in new_emails]\n",
    "\n",
    "# Convert text to TF-IDF\n",
    "X_new = vectorizer.transform(clean_new_emails)\n",
    "\n",
    "# Prediction probabilities\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "# Lower threshold for testing aggressive spam detection\n",
    "threshold = 0.25\n",
    "custom_predictions = (probs[:, 1] > threshold).astype(int)\n",
    "\n",
    "# Display results\n",
    "for email, prob, pred in zip(new_emails, probs, custom_predictions):\n",
    "    label = \"SPAM\" if pred == 1 else \"HAM\"\n",
    "    print(f\"Email: {email}\")\n",
    "    print(f\"HAM probability : {prob[0]:.4f}\")\n",
    "    print(f\"SPAM probability: {prob[1]:.4f}\")\n",
    "    print(f\"Final Prediction: {label}\")\n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a8894",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ea245",
   "metadata": {},
   "source": [
    "- A spam email detection system was successfully built using Natural Language Processing (NLP) and Machine Learning techniques on the Enron email dataset.\n",
    "\n",
    "- Text preprocessing steps such as lowercasing, tokenization, stopword removal, and duplicate handling significantly improved data quality and model reliability.\n",
    "\n",
    "- TF-IDF vectorization effectively transformed raw email text into meaningful numerical features for classification.\n",
    "\n",
    "- The trained model achieved a high accuracy of 97.86%, demonstrating strong overall performance in distinguishing spam from non-spam emails.\n",
    "\n",
    "- The model obtained a perfect recall (1.00) for spam emails, ensuring that no spam messages were missed, which is critical for real-world email filtering systems.\n",
    "\n",
    "- Precision for spam classification was 0.86, indicating that a small number of legitimate emails were incorrectly classified as spam.\n",
    "\n",
    "- The F1-score of 0.92 for spam class shows a strong balance between precision and recall.\n",
    "\n",
    "- The weighted average F1-score of 0.98 confirms that the model performs consistently well across both classes.\n",
    "\n",
    "- Proper data cleaning (null value and duplicate removal) helped prevent data leakage and inflated accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
